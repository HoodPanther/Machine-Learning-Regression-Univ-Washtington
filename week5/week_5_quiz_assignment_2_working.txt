Implementing LASSO using coordinate descent

8 questions
1
From the section "Effect of L1 penalty": Consider the simple model with 2 features trained on the entire sales dataset.

Which of the following values of l1_penalty would not set w[1] zero, but would set w[2] to zero, if we were to take a coordinate gradient step in that coordinate? (Select all that apply)

1.4e8 [false]

1.64e8 [true]

1.73e8 [true]

1.9e8 [false]

2.3e8 [false]

[CORRECT]
------------------------------------------------------------------------------------------------------------
NB: quiz question in python notebook is different.
"What range of values of l1_penalty would set both w[1] and w[2] to zero, if we were to take a step in that coordinate?"

for ro[0] =  87939470.773 , l1_penalty must be <  175878941.546  to cause w[1] to NOT be set to zero
for ro[0] =  87939470.773 , l1_penalty must be >  175878941.546  to cause w[1] to be set to zero
for ro[1] =  80966698.676 , l1_penalty must be >  161933397.352  to cause w[2] to be set to zero
we need l1_penalty in the range  1.62E+08  < l1_penalty <  1.76E+08

Q : Which of the following values of l1_penalty would not set w[1] zero, but would set w[2] to zero, 
l1_penalty must be >  175878941.546  w[1] becomes zero
l1_penalty must be >  161933397.352  w[2] becomes zero

so if             161933397.352 < l1_penalty < 175878941.546
so if                    1.61e8 < l1_penalty < 1.75e8

then w[1] will not be zero and w[2] will be zero.
------------------------------------------------------------------------------------------------------------
2. 
Refer to the same model as the previous question.

Which of the following values of l1_penalty would set both w[1] and w[2] to zero, if we were to take a coordinate gradient step in that coordinate? (Select all that apply)

1.4e8 [false]

1.64e8 [false]

1.73e8 [false]

1.9e8 [true]

2.3e8 [true]

[CORRECT]
------------------------------------------------------------------------------------------------------------
WANT : set both w[1] and w[2] to zero

for ro[0] =  87939470.773 , l1_penalty must be >  175878941.546  to cause w[1] to be set to zero
for ro[1] =  80966698.676 , l1_penalty must be >  161933397.352  to cause w[2] to be set to zero

need : l1_penalty must be >  175878941.546 [1.75e8] to set both  w[1] and w[2] to zero
------------------------------------------------------------------------------------------------------------
3. 
From the section "Cyclical coordinate descent": Using the simple model (with 2 features), we run our implementation of LASSO coordinate descent on the normalized sales dataset. We apply an L1 penalty of 1e7 and tolerance of 1.0.

Which of the following ranges contains the RSS of the learned model on the normalized dataset?

Between 8e13 and 2e14

Between 2e14 and 5e14

Between 5e14 and 8e14

Between 8e14 and 1e15

Between 1e15 and 3e15 [answer]  [CORRECT]
------------------------------------------------------------------------------------------------------------
weights = [ 21624998.36636292  63157246.78545421         0.        ]
rss = 1.63049248148e+15
simple_features = ['sqft_living', 'bedrooms']
weights column names = ['intercept', sqft_living', 'bedrooms']
------------------------------------------------------------------------------------------------------------
4. 
Refer to the same model as the previous question.

Which of the following features were assigned a zero weight at convergence?

constant  [false]

sqft_living  [false]

bedrooms  [true]

[CORRECT]
------------------------------------------------------------------------------------------------------------
constant  =  21624994.3932
sqft_living  =  63157250.4201
bedrooms  =  0.0
------------------------------------------------------------------------------------------------------------
5. 
In the section "Evaluating LASSO fit with more features", we split the data into training and test sets and learn weights with varying degree of L1 penalties. The model now has 13 features.

In the model trained with l1_penalty=1e7, which of the following features has non-zero weight? (Select all that apply)

constant [non zero]

sqft_living [non zero]

grade [zero]

waterfront [non zero]

sqft_basement [zero]

[CORRECT]
------------------------------------------------------------------------------------------------------------
which of the following features has non-zero weight? 

weights1e7 as calculated.
[ 24429600.60933314         0.                 0.          48389174.35227978
         0.                 0.           3317511.16271981   7329961.9848964
         0.                 0.                 0.                 0.
         0.                 0.        ]

all_features = ['bedrooms',
                'bathrooms',
                'sqft_living',
                'sqft_lot',
                'floors',
                'waterfront', 
                'view', 
                'condition', 
                'grade',
                'sqft_above',
                'sqft_basement',
                'yr_built', 
                'yr_renovated']
non zero feature weights
i= 2 ,  sqft_living  =  48389174.3523
i= 5 ,  waterfront  =  3317511.16272
i= 6 ,  view  =  7329961.9849


------------------------------------------------------------------------------------------------------------
6. 
This question refers to the same model as the previous question.

In the model trained with l1_penalty=1e8, which of the following features has non-zero weight? (Select all that apply)

constant [non zero]

sqft_living

grade

waterfront

sqft_basement

[CORRECT]
------------------------------------------------------------------------------------------------------------
NB: __ALL__ feature weights are ZERO!!!! only the intercept (constant) is non zero.
This is significant. do not underestimate the importance of this.

[ 71114625.75280938         0.                 0.                 0.
         0.                 0.                 0.                 0.
         0.                 0.                 0.                 0.
         0.                 0.        ]
------------------------------------------------------------------------------------------------------------
7. 
This question refers to the same model as the previous question.

In the model trained with l1_penalty=1e4, which of the following features has non-zero weight? (Select all that apply)

constant  [non zero]

sqft_living  [non zero]

grade   [non zero]

waterfront   [non zero]

sqft_basement   [non zero]

[CORRECT]
------------------------------------------------------------------------------------------------------------
weights1e4 = [ 77779073.91265222 -22884012.25023359  15348487.08089997
  92166869.69883074  -2139328.0824278   -8818455.54409495
   6494209.73310655   7065162.05053197   4119079.21006764  18436483.5261878
 -14566678.54514342  -5528348.75179427 -83591746.20730537
   2784276.46012858]
recall 
all_features = ['bedrooms',
                'bathrooms',
                'sqft_living',
                'sqft_lot',
                'floors',
                'waterfront', 
                'view', 
                'condition', 
                'grade',
                'sqft_above',
                'sqft_basement',
                'yr_built', 
                'yr_renovated']

------------------------------------------------------------------------------------------------------------
8. 
In the section "Evaluating each of the learned models on the test data", we evaluate three models on the test data. The three models were trained with same set of features but different L1 penalties.

Which of the three models gives the lowest RSS on the TEST data?

The model trained with 1e4

The model trained with 1e7

The model trained with 1e8
------------------------------------------------------------------------------------------------------------
"lowest RSS" answer = The model trained with 1e4  [CORRECT]


rss for normalized_weights1e4 =  2.2778100476e+14
rss for normalized_weights1e7 =  2.75962079909e+14
rss for normalized_weights1e8 =  5.37166150034e+14
------------------------------------------------------------------------------------------------------------

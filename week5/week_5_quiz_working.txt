Feature Selection and Lasso

7 questions
1
point
1. 
The best fit model of size 5 (i.e., with 5 features) always contains the set of features from best fit model of size 4.

True

False
--------------------------------------------------------------------------------
FALSE [CORRECT]
need to find lecture reference on this.
--------------------------------------------------------------------------------
2. 
Given 20 potential features, how many models do you have to evaluate in the all subsets algorithm?

Enter answer here
--------------------------------------------------------------------------------
2^D = 2^20 = 1,048,576 = 1048576[CORRECT]
--------------------------------------------------------------------------------
3. 
Given 20 potential features, how many models do you have to evaluate if you are running the forward stepwise greedy algorithm? Assume you run the algorithm all the way to the full feature set.

Enter answer here
--------------------------------------------------------------------------------
sum([D-i for i in range(0,20)]) [CORRECT]
answer = 210
refer lecture complexity of the forward stepwise algorythm.
https://www.coursera.org/learn/ml-regression/lecture/RRah3/complexity-of-the-greedy-forward-stepwise-algorithm
--------------------------------------------------------------------------------
4. 
Which of the plots could correspond to a lasso coefficient path? Select ALL that apply.

Hint: notice λ=∞ in the bottom right of the plots. How should coefficients behave eventually as λ goes to infinity?

--------------------------------------------------------------------------------
refer lecture the-lasso-objective-and-its-coefficient-path
https://www.coursera.org/learn/ml-regression/lecture/mw2Ul/the-lasso-objective-and-its-coefficient-path
@ 3:53 onwards
slide 52=ridge regression & slide 53=lasso regression
nb: when co-efficients of features drop to zero in lasso regression, we get graph on slide 53 where lines plotting coefficient converge to zero.
NB : first graph does not converge to zero as lamba approaches infinity.
2nd graph converges to zero @ lambda=infinity. kick away from zero is odd - the convergence to zero @ lambda=infinity is important.
3rd graph converges to zero as lambda approaches infinity. one coefficient converges to zero then spikes to non zero peak and converges to zero as lambda approaches infinity.
4th graph - one co-efficient does not converge to zero @ lambda=infinity, cannot be a lasso coefficient path.

selected 2nd and 3rd graph [CORRECT]

--------------------------------------------------------------------------------
5. 
Which of the following statements about coordinate descent is true? (Select all that apply.)

A small enough step size should be chosen to guarantee convergence.

To test the convergence of coordinate descent, look at the size of the maximum step you take as you cycle through coordinates.

Coordinate descent cannot be used to optimize the ordinary least squares objective.

Coordinate descent is always less efficient than gradient descent, but is often easier to implement.
--------------------------------------------------------------------------------
NB: in lecture 'coordinate-descent' we are told 'coordinate descent' does not use step size. thus several answers are elimintated. ie
[canot be true since no step size]  A small enough step size should be chosen to guarantee convergence.
[canot be true since no step size]  To test the convergence of coordinate descent, look at the size of the maximum step you take as you cycle through coordinates.
[should be possible to optimise the ordinary least squares objective, thus FALSE] Coordinate descent cannot be used to optimize the ordinary least squares objective.
[true] Coordinate descent is always less efficient than gradient descent, but is often easier to implement.
[attempt 1 : ABOVE is WRONG]


selected 3&4th option. [WRONG]
selected  4th option. [???]
--------------------------------------------------------------------------------
6. 
Using normalized features, the ordinary least squares coordinate descent update for feature j has the form (with ρj defined as in the videos):

w^j=ρj

w^j=(ρj)2

w^j=ρj−λ

w^j=ρj/2−λ
--------------------------------------------------------------------------------
w^j=ρj [CORRECT] need to explain why!

refer lecture "Coordinate descent for least squares regression (normalized features)"
https://www.coursera.org/learn/ml-regression/lecture/wkbZU/coordinate-descent-for-least-squares-regression-normalized-features

refer slide 78. lecture "coordinate-descent-for-least-squares-regression-normalized-features" @ 6'30"

--------------------------------------------------------------------------------
7. 
Using normalized features, the ridge regression coordinate descent update for feature j has the form (with ρj defined as in the videos):

w^j=ρj−λ

w^j=ρj/2−λ

w^j=ρj/(λ+1)

w^j=ρj
--------------------------------------------------------------------------------
w^j=ρj−λ [WRONG]
w^j=ρj/2−λ [WRONG]
w^j=ρj/(λ+1) [WRONG]
w^j=ρj  [WRONG]
all possible answers are wrong = WTF????
--------------------------------------------------------------------------------
